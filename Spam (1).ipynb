{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuDhLMxbPRzh"
      },
      "source": [
        "# **Importing Important Libraries**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MuaydlqpBIeI"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "qvnso8PjC8N3",
        "outputId": "9d5c50d6-8bb4-4373-d908-2f5258ed8974"
      },
      "source": [
        "df = pd.DataFrame() #making the DataFrame\n",
        "df = pd.read_csv('/content/spam.csv',encoding='iso-8859-1') #Extracting the Dataset from csv file\n",
        "df.head(20) #checking first 20 datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ham</td>\n",
              "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>spam</td>\n",
              "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>spam</td>\n",
              "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to tha...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>spam</td>\n",
              "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>ham</td>\n",
              "      <td>Oh k...i'm watching here:)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ham</td>\n",
              "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ham</td>\n",
              "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>spam</td>\n",
              "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      v1  ... Unnamed: 4\n",
              "0    ham  ...        NaN\n",
              "1    ham  ...        NaN\n",
              "2   spam  ...        NaN\n",
              "3    ham  ...        NaN\n",
              "4    ham  ...        NaN\n",
              "5   spam  ...        NaN\n",
              "6    ham  ...        NaN\n",
              "7    ham  ...        NaN\n",
              "8   spam  ...        NaN\n",
              "9   spam  ...        NaN\n",
              "10   ham  ...        NaN\n",
              "11  spam  ...        NaN\n",
              "12  spam  ...        NaN\n",
              "13   ham  ...        NaN\n",
              "14   ham  ...        NaN\n",
              "15  spam  ...        NaN\n",
              "16   ham  ...        NaN\n",
              "17   ham  ...        NaN\n",
              "18   ham  ...        NaN\n",
              "19  spam  ...        NaN\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "ErtobTPVFeqj",
        "outputId": "92b9f01a-31bd-4869-fc79-9721c4b2539f"
      },
      "source": [
        "df.tail(20) #checking last 20 datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5552</th>\n",
              "      <td>ham</td>\n",
              "      <td>Have a safe trip to Nigeria. Wish you happines...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5553</th>\n",
              "      <td>ham</td>\n",
              "      <td>Hahaha..use your brain dear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5554</th>\n",
              "      <td>ham</td>\n",
              "      <td>Well keep in mind I've only got enough gas for...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5555</th>\n",
              "      <td>ham</td>\n",
              "      <td>Yeh. Indians was nice. Tho it did kane me off ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5556</th>\n",
              "      <td>ham</td>\n",
              "      <td>Yes i have. So that's why u texted. Pshew...mi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5557</th>\n",
              "      <td>ham</td>\n",
              "      <td>No. I meant the calculation is the same. That ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5558</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>ham</td>\n",
              "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hou...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5560</th>\n",
              "      <td>ham</td>\n",
              "      <td>Anything lor. Juz both of us lor.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5561</th>\n",
              "      <td>ham</td>\n",
              "      <td>Get me out of this dump heap. My mom decided t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5562</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lor... Sony ericsson salesman... I ask shuh...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5563</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ard 6 like dat lor.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5564</th>\n",
              "      <td>ham</td>\n",
              "      <td>Why don't you wait 'til at least wednesday to ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5565</th>\n",
              "      <td>ham</td>\n",
              "      <td>Huh y lei...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5566</th>\n",
              "      <td>spam</td>\n",
              "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        v1  ... Unnamed: 4\n",
              "5552   ham  ...        NaN\n",
              "5553   ham  ...        NaN\n",
              "5554   ham  ...        NaN\n",
              "5555   ham  ...        NaN\n",
              "5556   ham  ...        NaN\n",
              "5557   ham  ...        NaN\n",
              "5558   ham  ...        NaN\n",
              "5559   ham  ...        NaN\n",
              "5560   ham  ...        NaN\n",
              "5561   ham  ...        NaN\n",
              "5562   ham  ...        NaN\n",
              "5563   ham  ...        NaN\n",
              "5564   ham  ...        NaN\n",
              "5565   ham  ...        NaN\n",
              "5566  spam  ...        NaN\n",
              "5567  spam  ...        NaN\n",
              "5568   ham  ...        NaN\n",
              "5569   ham  ...        NaN\n",
              "5570   ham  ...        NaN\n",
              "5571   ham  ...        NaN\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wM4FduyE4R7",
        "outputId": "eb9f56a8-5610-4bbf-e0e2-c9c1616b7ac2"
      },
      "source": [
        "df.shape #checking the shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 5)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QijV49rcQ0gC"
      },
      "source": [
        "▶ We can see there are **5572** data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiXYcX25Esmu",
        "outputId": "5b9a2bd8-265e-4f6e-a006-588e7098745f"
      },
      "source": [
        "df.info() #checking the information of the data set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   v1          5572 non-null   object\n",
            " 1   v2          5572 non-null   object\n",
            " 2   Unnamed: 2  50 non-null     object\n",
            " 3   Unnamed: 3  12 non-null     object\n",
            " 4   Unnamed: 4  6 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEds4eW3RMKn"
      },
      "source": [
        "▶ As per information there are **5 object** type data in the dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2wyc1gDRlYT"
      },
      "source": [
        "\n",
        "\n",
        "▶ We don't need **Unnamed 2**, **Unnamed 3**, **Unnamed 4** column\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "because those columns are full of **NaN values**. So we will **drop** \n",
        "\n",
        "---\n",
        "\n",
        "all those columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dHYpR5UoFOp1"
      },
      "source": [
        "df=df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E9PB6nxFq8G",
        "outputId": "448cb4f9-b5c0-4303-8dfb-00968619e4dd"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   v1      5572 non-null   object\n",
            " 1   v2      5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTgvG0UDSOdb"
      },
      "source": [
        "▶ We still have **2 object** type data that we have to change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKgrP4QFFs74",
        "outputId": "680d7280-b00f-41a9-c6ef-df4a4aa6d272"
      },
      "source": [
        "df['v1'].value_counts() #checking the value counts of column 'v1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: v1, dtype: int64"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UwSwReUsGn6w",
        "outputId": "f2b301b4-8402-46de-a223-18f15eae6a7d"
      },
      "source": [
        "mail={'ham':0,'spam':1}\n",
        "df['v1'] = df['v1'].map(mail)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      v1                                                 v2\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                      Ok lar... Joking wif u oni...\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro...\n",
              "...   ..                                                ...\n",
              "5567   1  This is the 2nd time we have tried 2 contact u...\n",
              "5568   0              Will Ì_ b going to esplanade fr home?\n",
              "5569   0  Pity, * was in mood for that. So...any other s...\n",
              "5570   0  The guy did some bitching but I acted like i'd...\n",
              "5571   0                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NBJd8p9G8Kd",
        "outputId": "84ae5d5f-f4c9-48ca-8949-6b032bfa835e"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   v1      5572 non-null   int64 \n",
            " 1   v2      5572 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 87.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8mPd5THUom9"
      },
      "source": [
        "▶ Now all we have to deal with the **text part** of column '**v2**'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xBSExsX9G-87"
      },
      "source": [
        "df['v2']=df['v2'].str.lower() #converting the text of column 'v2' into lower case letter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDCqGQOCVQal"
      },
      "source": [
        "Importing Some more Important libraries like **re**, **string**, **nltk**, **Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HinXQX2zHVms",
        "outputId": "62841c80-52e7-4504-87cb-82c3f69377f5"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords') #it will download all the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "swords=stopwords.words('english') #saving all the stopwords as swords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw8dcNNsHr4g",
        "outputId": "1f06bc39-b17f-41f3-bf72-ff22b0d4f107"
      },
      "source": [
        "swords #let's check the stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDaexo0zIcAC",
        "outputId": "8126bba1-c718-42ba-a2e4-b6091b12085a"
      },
      "source": [
        "def remove_stopwords(v2):\n",
        "    return \" \".join([word for word in str(v2).split() if word not in swords]) #making the function that will remove all the stopwords from column 'v2'\n",
        "\n",
        "df['v2'] = df['v2'].apply(lambda v2: remove_stopwords(v2)) #calling the function here\n",
        "df['v2'].head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     go jurong point, crazy.. available bugis n gre...\n",
              "1                         ok lar... joking wif u oni...\n",
              "2     free entry 2 wkly comp win fa cup final tkts 2...\n",
              "3             u dun say early hor... u c already say...\n",
              "4               nah think goes usf, lives around though\n",
              "5     freemsg hey darling 3 week's word back! i'd li...\n",
              "6     even brother like speak me. treat like aids pa...\n",
              "7     per request 'melle melle (oru minnaminunginte ...\n",
              "8     winner!! valued network customer selected rece...\n",
              "9     mobile 11 months more? u r entitled update lat...\n",
              "10    i'm gonna home soon want talk stuff anymore to...\n",
              "11    six chances win cash! 100 20,000 pounds txt> c...\n",
              "12    urgent! 1 week free membership å£100,000 prize...\n",
              "13    i've searching right words thank breather. pro...\n",
              "14                                   date sunday will!!\n",
              "15    xxxmobilemovieclub: use credit, click wap link...\n",
              "16                           oh k...i'm watching here:)\n",
              "17    eh u remember 2 spell name... yes did. v naugh...\n",
              "18          fine thatåõs way u feel. thatåõs way gota b\n",
              "19    england v macedonia - dont miss goals/team new...\n",
              "Name: v2, dtype: object"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOrWaR5gXKpU"
      },
      "source": [
        "Now we can see there are lots of punctuations in the text part that we \n",
        "\n",
        "---\n",
        "\n",
        "have to remove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1gz0OxBJq05",
        "outputId": "14ad6719-a8f0-4e32-ce4e-4fb8467a282a"
      },
      "source": [
        "def remove_punctuations(v2):  #making a function that will remove the punctuations\n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return v2.translate(translator)\n",
        "\n",
        "df['v2']= df['v2'].apply(lambda x: remove_punctuations(x)) #calling the function\n",
        "df['v2'].head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     go jurong point crazy available bugis n great ...\n",
              "1                               ok lar joking wif u oni\n",
              "2     free entry 2 wkly comp win fa cup final tkts 2...\n",
              "3                   u dun say early hor u c already say\n",
              "4                nah think goes usf lives around though\n",
              "5     freemsg hey darling 3 weeks word back id like ...\n",
              "6     even brother like speak me treat like aids patent\n",
              "7     per request melle melle oru minnaminunginte nu...\n",
              "8     winner valued network customer selected receiv...\n",
              "9     mobile 11 months more u r entitled update late...\n",
              "10    im gonna home soon want talk stuff anymore ton...\n",
              "11    six chances win cash 100 20000 pounds txt csh1...\n",
              "12    urgent 1 week free membership å£100000 prize j...\n",
              "13    ive searching right words thank breather promi...\n",
              "14                                     date sunday will\n",
              "15    xxxmobilemovieclub use credit click wap link n...\n",
              "16                                 oh kim watching here\n",
              "17    eh u remember 2 spell name yes did v naughty m...\n",
              "18           fine thatåõs way u feel thatåõs way gota b\n",
              "19    england v macedonia  dont miss goalsteam news ...\n",
              "Name: v2, dtype: object"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d11W7yfbYiZq"
      },
      "source": [
        "As it is **Spam mail** Dataset. So there is possibility of the presence of different **url** of **websites**. So we have to remove all those urls for the sake of trainning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ2oiqQPLOeS",
        "outputId": "290a3a56-e682-48cc-c83b-a48493d203ae"
      },
      "source": [
        "def remove_URLs(data):  #making the function that will remove all the URL\n",
        "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
        "\n",
        "df['v2'] = df['v2'].apply(lambda x: remove_URLs(x)) #calling the function\n",
        "df['v2'].tail(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5552    safe trip nigeria wish happiness soon company ...\n",
              "5553                                 hahahause brain dear\n",
              "5554    well keep mind ive got enough gas one round tr...\n",
              "5555    yeh indians nice tho kane bit he shud go 4 dri...\n",
              "5556            yes have thats u texted pshewmissing much\n",
              "5557    no meant calculation same ltgt units ltgt  sch...\n",
              "5558                                 sorry ill call later\n",
              "5559                       next ltgt hours imma flip shit\n",
              "5560                              anything lor juz us lor\n",
              "5561          get dump heap mom decided come lowes boring\n",
              "5562    ok lor sony ericsson salesman ask shuhui say q...\n",
              "5563                                   ard 6 like dat lor\n",
              "5564                    wait til least wednesday see get \n",
              "5565                                              huh lei\n",
              "5566    reminder o2 get 250 pounds free call credit de...\n",
              "5567    2nd time tried 2 contact u u å£750 pound prize...\n",
              "5568                          ì b going esplanade fr home\n",
              "5569                    pity  mood that soany suggestions\n",
              "5570    guy bitching acted like id interested buying s...\n",
              "5571                                       rofl true name\n",
              "Name: v2, dtype: object"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wMqS9XqLwRr",
        "outputId": "a35e103b-4382-4456-de5c-306ea239a446"
      },
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "    \n",
        "df['v2'] = df['v2'].apply(lambda x: remove_numbers(x))\n",
        "df['v2'].head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     go jurong point crazy available bugis n great ...\n",
              "1                               ok lar joking wif u oni\n",
              "2     free entry  wkly comp win fa cup final tkts st...\n",
              "3                   u dun say early hor u c already say\n",
              "4                nah think goes usf lives around though\n",
              "5     freemsg hey darling  weeks word back id like f...\n",
              "6     even brother like speak me treat like aids patent\n",
              "7     per request melle melle oru minnaminunginte nu...\n",
              "8     winner valued network customer selected receiv...\n",
              "9     mobile  months more u r entitled update latest...\n",
              "10    im gonna home soon want talk stuff anymore ton...\n",
              "11    six chances win cash   pounds txt csh send  co...\n",
              "12    urgent  week free membership å£ prize jackpot ...\n",
              "13    ive searching right words thank breather promi...\n",
              "14                                     date sunday will\n",
              "15    xxxmobilemovieclub use credit click wap link n...\n",
              "16                                 oh kim watching here\n",
              "17    eh u remember  spell name yes did v naughty ma...\n",
              "18           fine thatåõs way u feel thatåõs way gota b\n",
              "19    england v macedonia  dont miss goalsteam news ...\n",
              "Name: v2, dtype: object"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YW_G_kvfQDUW"
      },
      "source": [
        "review_lines = list()\n",
        "lines = df['v2'].values.tolist()\n",
        "words = [word for word in lines if word.isalpha()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X27BdadaL722"
      },
      "source": [
        "mail = df['v2'].tolist()\n",
        "subs = df['v1'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPlLGQ46MzQD",
        "outputId": "7f714938-4113-41b6-8a2b-020e5f020281"
      },
      "source": [
        "print(mail[57])\n",
        "print(subs[57])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sorry ill call later meeting\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8jMZqYoNUsS",
        "outputId": "44fb7a1a-cf67-421c-a81d-992cbacb527b"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(mail)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "size_of_vocab=len(word_index) + 1 \n",
        "print(\"Vocab Size = %d\"%size_of_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size = 8549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEB-4GJlOic4",
        "outputId": "0993d8b5-280d-4948-a1fa-9cb73dd22d0e"
      },
      "source": [
        "indices = np.arange(len(mail))\n",
        "print(indices)\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   0    1    2 ... 5569 5570 5571]\n",
            "[2577 1126 1482 ... 2752 2282  899]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FvM-GjL_Pfdu"
      },
      "source": [
        "mail=np.array(mail)[indices.astype(int)]\n",
        "subs=np.array(subs)[indices.astype(int)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aRQC99fOP80i"
      },
      "source": [
        "train_size = int(len(mail) * 0.9)\n",
        "\n",
        "train_mail = mail[0: train_size]\n",
        "test_mail = mail[train_size:]\n",
        "train_subs = subs[0: train_size]\n",
        "test_subs = subs[train_size:]\n",
        "\n",
        "training_subs_final = np.array(train_subs)\n",
        "testing_subs_final = np.array(test_subs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "itYj0Y_HRcmn",
        "outputId": "56a0f399-fc2b-4a39-b0fd-3aaff0a69c69"
      },
      "source": [
        "print(\"Shape of training dataset X : \"+str(len(train_mail)))\n",
        "print(\"Shape of training dataset Y : \"+str(len(training_subs_final)))\n",
        "print(\"Shape of testing dataset X : \"+str(len(test_mail)))\n",
        "print(\"Shape of testing dataset Y : \"+str(len(testing_subs_final)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training dataset X : 5014\n",
            "Shape of training dataset Y : 5014\n",
            "Shape of testing dataset X : 558\n",
            "Shape of testing dataset Y : 558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R_Mid8KNSBe4"
      },
      "source": [
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jLqQ9FYCSO4F"
      },
      "source": [
        "training_sequences = tokenizer.texts_to_sequences(train_mail)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_mail)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mobKj75TQcu",
        "outputId": "19d23158-2a00-4552-e5fe-f15d88cce3a5"
      },
      "source": [
        "print(train_mail[20])\n",
        "print(train_subs[20])\n",
        "training_padded[20]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "watching cartoon listening music amp eve go temple amp church u\n",
            "0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 253, 1562, 1563,  430,   90,  519,    6, 1826,   90, 1827,    1,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0], dtype=int32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmfn-9EkTVVP",
        "outputId": "d9e37f8e-5f68-4b46-eb44-09b7ed3fc8d6"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/glove.twitter.27B.100d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1193514 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xxfqqfWayRCM"
      },
      "source": [
        "embedding_matrix = np.zeros((size_of_vocab, 100))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8bKLL-PyVAz",
        "outputId": "6095152e-a907-475f-8298-cda5e894d68f"
      },
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "embedding_layer = Embedding(size_of_vocab, embedding_dim, weights = [embedding_matrix], input_length = max_length, trainable = False)\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(LSTM(200, dropout = 0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1,min_lr = 0.01,verbose=1)\n",
        "\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          854900    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 200)               240800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                12864     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,108,629\n",
            "Trainable params: 253,729\n",
            "Non-trainable params: 854,900\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdOQ97FDzrgq",
        "outputId": "4e786df1-11e7-40d5-e1ba-714c5bbbd5bb"
      },
      "source": [
        "history = model.fit(training_padded,training_subs_final,batch_size=1024,epochs=15,validation_data=(testing_padded,testing_subs_final),verbose=1,callbacks=[es,mc,reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6746 - acc: 0.8644\n",
            "Epoch 00001: val_acc improved from -inf to 0.85842, saving model to best_model.h5\n",
            "5/5 [==============================] - 7s 339ms/step - loss: 0.6746 - acc: 0.8644 - val_loss: 0.6120 - val_acc: 0.8584 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5334 - acc: 0.8668\n",
            "Epoch 00002: val_acc did not improve from 0.85842\n",
            "5/5 [==============================] - 1s 165ms/step - loss: 0.5334 - acc: 0.8668 - val_loss: 0.4060 - val_acc: 0.8584 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4544 - acc: 0.8668\n",
            "Epoch 00003: val_acc did not improve from 0.85842\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.4544 - acc: 0.8668 - val_loss: 0.4355 - val_acc: 0.8584 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3133 - acc: 0.8773\n",
            "Epoch 00004: val_acc improved from 0.85842 to 0.95161, saving model to best_model.h5\n",
            "5/5 [==============================] - 1s 175ms/step - loss: 0.3133 - acc: 0.8773 - val_loss: 0.1829 - val_acc: 0.9516 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2101 - acc: 0.9477\n",
            "Epoch 00005: val_acc did not improve from 0.95161\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.2101 - acc: 0.9477 - val_loss: 0.2092 - val_acc: 0.9516 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2140 - acc: 0.9202\n",
            "Epoch 00006: val_acc did not improve from 0.95161\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 0.2140 - acc: 0.9202 - val_loss: 0.2131 - val_acc: 0.9086 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1894 - acc: 0.9234\n",
            "Epoch 00007: val_acc improved from 0.95161 to 0.95878, saving model to best_model.h5\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.1894 - acc: 0.9234 - val_loss: 0.1810 - val_acc: 0.9588 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1877 - acc: 0.9136\n",
            "Epoch 00008: val_acc did not improve from 0.95878\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.1877 - acc: 0.9136 - val_loss: 0.2144 - val_acc: 0.8961 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1907 - acc: 0.9085\n",
            "Epoch 00009: val_acc did not improve from 0.95878\n",
            "5/5 [==============================] - 1s 160ms/step - loss: 0.1907 - acc: 0.9085 - val_loss: 0.1916 - val_acc: 0.9480 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1907 - acc: 0.9440\n",
            "Epoch 00010: val_acc did not improve from 0.95878\n",
            "5/5 [==============================] - 1s 161ms/step - loss: 0.1907 - acc: 0.9440 - val_loss: 0.2587 - val_acc: 0.9355 - lr: 0.0010\n",
            "Epoch 00010: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPRgW-V7z3Qk"
      },
      "source": [
        "with open('model_architecture_Spam_classifier.json','w') as f:\n",
        "    f.write(model.to_json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rav7P5ZE0eDi"
      },
      "source": [
        "model.save_weights('Spam_Classifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OXOEfdR0knU"
      },
      "source": [
        "test_sample_1 = \"How are you? need to talk\"\n",
        "test_sample_2 = \"You won’t believe this,A doctor in Arizona has discovered the root-cause of high blood sugar.It has nothing to do with eating carbs or sugar….But with “Beta Cells” inside the pancreas…Watch his video while it’s still up…Stay safe,Your Name\"\n",
        "\n",
        "test_samples = [test_sample_1, test_sample_2]\n",
        "\n",
        "test_samples=[i.lower() for i in test_samples]\n",
        "test_samples=[remove_stopwords(i) for i in test_samples]\n",
        "test_samples=[remove_punctuations(i) for i in test_samples]\n",
        "test_samples=[remove_URLs(i) for i in test_samples]\n",
        "test_samples=[remove_numbers(i) for i in test_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp0wXPgufk77",
        "outputId": "2879624d-82af-48d7-c0a3-de1ae894c842"
      },
      "source": [
        "test_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['you need talk',\n",
              " 'won’t believe thisa doctor arizona discovered rootcause high blood sugarit nothing eating carbs sugar…but “beta cells” inside pancreas…watch video it’s still up…stay safeyour name']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZYNbGZefpbT",
        "outputId": "286a7da1-62ce-470b-c95d-caeb9594a8ba"
      },
      "source": [
        "test_samples_tokens = tokenizer.texts_to_sequences(test_samples)\n",
        "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
        "\n",
        "#predict\n",
        "model.predict(x=test_samples_tokens_pad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.02864225],\n",
              "       [0.02889125]], dtype=float32)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J0ctqJnOfvTM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}